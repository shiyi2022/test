# -*- coding: utf-8 -*-
"""test

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X1r-nUtkIRVWVGnYOfngjUYDCWrqiHhb

# Import data
"""

!pip install pyspark

from google.colab import drive
drive.mount('/content/drive')

from pyspark.sql import SparkSession
spark = SparkSession \
    .builder \
    .appName("COMP5349 A2") \
    .getOrCreate()

from pyspark import SparkConf, SparkContext

spark_conf = SparkConf()\
        .setAppName("assignment2")
sc=SparkContext.getOrCreate(spark_conf) 

test_data = "/content/drive/MyDrive/comp5349/a2_data/test.json"
test_init_df = spark.read.json(test_data)

"""#Data prepare-展平嵌套数据"""

test_init_df.printSchema()

from pyspark.sql.functions import explode
test_data_df= test_init_df.select((explode("data").alias('data')))

test_data_df.printSchema()

test_data_df

test_paragraphs_df= test_data_df.select('data.title',(explode("data.paragraphs").alias('paragraphs')))

test_paragraphs_df.printSchema()

test_paragraphs_df.count()

#test_qas_df= test_paragraphs_df.select(explode("paragraphs.qas").alias('qas'))

df= test_paragraphs_df.select('title',"paragraphs.context", explode("paragraphs.qas").alias('qas'))



df.printSchema()

df.count()

df2= df.select('title',"context",'qas.question','qas.is_impossible','qas.id','qas.answers')

df2.printSchema()

df2.count()

"""# 样本分类 """

#样本分类 ture, false;

# negative samples 
##is_impossible=True,没有答案，impossible negative



df2_ture=df2.where("is_impossible=True").select('title',"context",'id','question','is_impossible')

df2_ture.printSchema()

# positive samples

df2_false=df2.where("is_impossible=False")#.select("context",'question','is_impossible', explode("answers").alias('answers'))

df2_false.printSchema()

df2_false.count()

df3_false=df2_false.select('title',"context",'id','question','is_impossible', explode("answers").alias('answers'))

df3_false.printSchema()

df3_false.count()

df4_false=df3_false.select('title',"context",'id','question','is_impossible', 'answers.answer_start','answers.text')

df4_false.count()

#df= df2_false.select("paragraphs.context", explode("paragraphs.qas").alias('qas'))



"""# 长文本分割

"""

##将文本分类，df2_ture， df3_false

#df2_ture

df2_ture.printSchema()

def split_rdd(line):
     
     title,comtext,id,question,is_impossible=line
     length=len(comtext)
     a=(length//2048)+1
     b=[]
     for i in range(a):
          c=[]
          if i*2048+4096<length:
             cc=comtext[i*2048:i*2048+4096]
             c.append(title)
             c.append(cc)
             c.append(i)
             c.append(id)
             c.append(question)
          else:
             cc=comtext[i*2048:length]
             c.append(title)
             c.append(cc)
             c.append(i)
             c.append(id)
             c.append(question)
          b.append(c)
     return b

a=df2_ture.rdd

a.take(4)

c=a.flatMap(split_rdd)

c.count()



#posible negative and positive

df4_false.printSchema()

aaa=df4_false.rdd

aaa.take(3)

def split_rdd2(line):
     
     title,context,id,question,is_impossible,answer_start,text=line
     length=len(context)
     a=(length//2048)+1
     length_text=len(text)
     start_index=max(0,(answer_start//2048)-1)
     end_index=(answer_start+length_text)//2048
     b=[]
     for i in range(a):
          c=[]
          if i*2048+4096<length:
             cc=context[i*2048:i*2048+4096]
             c.append(title)
             c.append(cc)
             c.append(i)
             c.append(id)
             c.append(question)
             c.append(start_index)
             c.append(end_index)
             c.append(answer_start)
             c.append(text)
          else:
             cc=context[i*2048:length]
             c.append(title)
             c.append(cc)
             c.append(i)
             c.append(id)
             c.append(question)
             c.append(start_index)
             c.append(end_index)
             c.append(answer_start)
             c.append(text)
          b.append(c)
     return b

max(0,5-6)

2056//2048

aaa.take(2)

ccc=aaa.flatMap(split_rdd2)

ccc.take(1)

ccc.count()

##positive

def positive(line):
     
     title,context,index,id,question,start_index,end_index,answer_start,text=line
     if start_index<=index and index<=end_index:
       return title,context,index,id,question,start_index,end_index,answer_start,text

ddd=ccc.map(positive)

ddd.take(100)

eee=ddd.filter( lambda x: x is not None)

eee.take(1)

eee.count()

def positive2(line):
     
     title,context,index,id,question,start_index,end_index,answer_start,text=line
     a=answer_start-index*2048
     b=a+len(text)
     answer_start=max(0,a)
     answer_end=min(b,4096)
     
     return title,context,index,id,question,answer_start,answer_end

fff=eee.map(positive2)

fff.count()



# possible negative

ccc.count()

ccc.take(1)

def possible_negative(line):
     
     title,context,index,id,question,start_index,end_index,answer_start,text=line
     if start_index>index or index>end_index:
       return title,context,index,id,question

ggg=ccc.map(possible_negative)

ggg.take(10)

possible_negative_samples=ggg.filter( lambda x: x is not None)

possible_negative_samples.count()



"""#平衡样本

"""

# positive  fff  4886
# possible negative  possible_negative_samples 98492
# impossible negative c 56664

samples_positive=fff.toDF()
samples_possible_negative=possible_negative_samples.toDF()
samples_impossible_negative=c.toDF()

samples_positive.toJSON().first()